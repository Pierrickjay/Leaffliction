{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "from plantcv import plantcv as pcv\n",
    "from plantcv.parallel import WorkflowInputs\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import experimental, Conv2D, MaxPooling2D, Dense, Flatten\n",
    "import os\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataset(path, img_size, batch_size):\n",
    "    return tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        path,\n",
    "        shuffle=True,\n",
    "        image_size=(img_size, img_size)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_corrupted_jpeg_array(img_array):\n",
    "    try:\n",
    "        # Tentez d'ouvrir l'image avec Pillow\n",
    "        img = Image.fromarray(img_array, mode=\"RGB\")\n",
    "        img.verify()  # Vérifiez l'intégrité du fichier JPEG\n",
    "        return (\"OK\")\n",
    "    except (IOError, SyntaxError) as e:\n",
    "        return (e)\n",
    "    \n",
    "def test_corrupted_jpeg(img):\n",
    "    try:\n",
    "        # Tentez d'ouvrir l'image avec Pillow\n",
    "        img.verify()  # Vérifiez l'intégrité du fichier JPEG\n",
    "        return (\"OK\")\n",
    "    except (IOError, SyntaxError) as e:\n",
    "        return (e)\n",
    "\n",
    "\n",
    "def test_corrupted_jpeg_path(image_path):\n",
    "    try:\n",
    "        # Tentez d'ouvrir l'image avec Pillow\n",
    "        img = Image.open(image_path)\n",
    "        img.verify()  # Vérifiez l'intégrité du fichier JPEG\n",
    "        print(\"L'image est valide.\")\n",
    "    except (IOError, SyntaxError) as e:\n",
    "        print(f\"L'image est corrompue : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeBack(img, size_fill, enhance_val, buffer_size):\n",
    "    img_img = Image.fromarray(img, mode=\"RGB\")\n",
    "    contr_img = ImageEnhance.Contrast(img_img).enhance(enhance_val)\n",
    "    gray_img = pcv.rgb2gray_lab(rgb_img=np.array(contr_img), channel='a')\n",
    "    thresh = pcv.threshold.triangle(\n",
    "        gray_img=gray_img, object_type=\"dark\", xstep=100)\n",
    "    edge_ok = pcv.fill(bin_img=thresh, size=5000)\n",
    "    mask = pcv.fill(bin_img=pcv.invert(gray_img=edge_ok), size=size_fill)\n",
    "    contours, _ = cv2.findContours(\n",
    "        mask, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    mask_buf = mask.copy()\n",
    "    if (len(contours)):\n",
    "        cv2.drawContours(mask_buf,\n",
    "                         contours[np.argmax([len(c) for c in contours])],\n",
    "                         -1, (0, 0, 0), buffer_size)\n",
    "    if ([mask_buf[0, 0], mask_buf[0, -1],\n",
    "         mask_buf[0, -1], mask_buf[-1, 0]] == [0, 0, 0, 0]):\n",
    "        mask_buf[0:11, 0:11] = 255\n",
    "        mask_buf[-11:, -11:] = 255\n",
    "        mask_buf[0:11, -11:] = 255\n",
    "        mask_buf[-11:, 0:11] = 255\n",
    "    mask_buf[0:1, :] = 255\n",
    "    mask_buf[-1:, :] = 255\n",
    "    mask_buf[:, 0:1] = 255\n",
    "    mask_buf[:, -1:] = 255\n",
    "    mask_buf = pcv.fill(bin_img=mask_buf, size=size_fill)\n",
    "    img_modified = np.ones_like(img) * 255\n",
    "    img_modified[mask_buf == 0] = img[mask_buf == 0]\n",
    "    return img_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processImgDataSet(path):\n",
    "    img_path_list = [\n",
    "         [[foldername, fn, '/'.join(\n",
    "              [e for e in foldername.split(\"/\") if e not in [\"..\", \".\"]])]\n",
    "          for fn in filenames]\n",
    "         for foldername, subdirectory, filenames in os.walk(path)\n",
    "         if len(filenames)]\n",
    "    img_path_list = np.array([element for sous_liste in\n",
    "                              img_path_list for element in sous_liste])\n",
    "    img_array = np.array(\n",
    "         [np.array(Image.open(str(img_path[0] + \"/\" + img_path[1]), \"r\"))\n",
    "          for img_path in img_path_list])\n",
    "    img_back_removed = [removeBack(img, 5000, 1, 10) for img in img_array]\n",
    "    img_back_removed_IMG = [Image.fromarray(img_array)\n",
    "                            for img_array in img_back_removed]\n",
    "    [os.makedirs(\"increased/\" + path[2], exist_ok=True)\n",
    "     for path in img_path_list]\n",
    "    [img.save(\"increased/\" + path[2] + \"/\" + path[1])\n",
    "     for path, img in zip(img_path_list, img_back_removed_IMG)]\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../leaves/\"\n",
    "img_path_list = [\n",
    "\t\t[[foldername, fn, '/'.join(\n",
    "\t\t\t[e for e in foldername.split(\"/\") if e not in [\"..\", \".\"]])]\n",
    "\t\tfor fn in filenames]\n",
    "\t\tfor foldername, subdirectory, filenames in os.walk(path)\n",
    "\t\tif len(filenames)]\n",
    "img_path_list = np.array([element for sous_liste in\n",
    "\t\t\t\t\t\t\timg_path_list for element in sous_liste])\n",
    "img_array = np.array(\n",
    "\t\t[np.array(Image.open(str(img_path[0] + \"/\" + img_path[1]), \"r\"))\n",
    "\t\tfor img_path in img_path_list])\n",
    "img_back_removed = [removeBack(img, 5000, 1, 10) for img in img_array]\n",
    "img_back_removed_IMG = [Image.fromarray(img_array)\n",
    "\t\t\t\t\t\tfor img_array in img_back_removed]\n",
    "[os.makedirs(\"increased/\" + path[2], exist_ok=True) for path in img_path_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path, img in zip(img_path_list, img_back_removed_IMG):\n",
    "\ttry:\n",
    "\t\t# Assurez-vous que le chemin de fichier est valide\n",
    "\t\tsave_path = os.path.join(\"increased\", path[2], path[1].split(\".\")[0] + \".png\")\n",
    "\t\t# Enregistrez l'image avec le bon format de fichier\n",
    "\t\timg.save(save_path, format=\"PNG\")  # Utilisez \"PNG\" pour les images PNG\n",
    "\t\tprint(f\"L'image a été enregistrée avec succès : {save_path}\")\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"Erreur lors de l'enregistrement de l'image : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_partition_tf(ds, train_split=0.85,\n",
    "                             shuffle=True, shuffle_size=10000):\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle_size, seed=12)\n",
    "    len_train_dataset = int(len(ds) * train_split)\n",
    "    train_dataset = ds.take(len_train_dataset)\n",
    "    cv_dataset = ds.skip(len_train_dataset)\n",
    "    return train_dataset, cv_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 15\n",
    "path = \"../leaves\"\n",
    "save_dir = \"\"\n",
    "save_name = \"learnings\"\n",
    "img_size = 256\n",
    "input_shape = (img_size, img_size, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "processImgDataSet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../leaves/\"\n",
    "img_path_list = [\n",
    "\t\t[[foldername, fn, '/'.join(\n",
    "\t\t\t[e for e in foldername.split(\"/\") if e not in [\"..\", \".\"]]),\n",
    "\t\t\tfoldername.split(\"/\")[-1]]\n",
    "\t\tfor fn in filenames]\n",
    "\t\tfor foldername, subdirectory, filenames in os.walk(path)\n",
    "\t\tif len(filenames)]\n",
    "img_path_list = np.array([element for sous_liste in\n",
    "\t\t\t\t\t\t\timg_path_list for element in sous_liste])\n",
    "img_array = np.array(\n",
    "\t\t[np.array(Image.open(str(img_path[0] + \"/\" + img_path[1]), \"r\"))\n",
    "\t\tfor img_path in img_path_list])\n",
    "img_back_removed = [np.array(removeBack(img, 5000, 1, 10)) / 255.0 for img in img_array]\n",
    "img_back_removed_IMG = [Image.fromarray(img_array)\n",
    "                            for img_array in img_back_removed]\n",
    "[os.makedirs(\"increased/\" + path[2], exist_ok=True)\n",
    "    for path in img_path_list]\n",
    "[img.save(os.path.join(\n",
    "    \"increased\", path[2], path[1].split(\".\")[0] + \".png\"), format=\"PNG\")\n",
    "    for path, img in zip(img_path_list, img_back_removed_IMG)]\n",
    "\n",
    "\n",
    "# def load_and_preprocess_image(image_path):\n",
    "#     # Charger l'image\n",
    "#     img = Image.open(image_path)\n",
    "#     # Prétraiter l'image (redimensionner, normaliser, etc.)\n",
    "#     img = np.array(img) / 255.0  # Normaliser les valeurs de pixel entre 0 et 1\n",
    "#     img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])  # Redimensionner l'image si nécessaire\n",
    "#     return img\n",
    "\n",
    "# # Liste des chemins vers les images et leurs étiquettes de classe correspondantes\n",
    "# image_paths = [\"image1.jpg\", \"image2.jpg\", \"image3.jpg\", ...]\n",
    "# labels = [0, 1, 2, ...]  # Liste des étiquettes de classe correspondant à chaque image\n",
    "\n",
    "# # Paramètres de redimensionnement des images\n",
    "# IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "\n",
    "# # Charger et prétraiter les images\n",
    "# image_data = [load_and_preprocess_image(image_path) for image_path in image_paths]\n",
    "\n",
    "# # Créer un ensemble de données TensorFlow à partir des images et de leurs étiquettes de classe\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((image_data, labels))\n",
    "\n",
    "# # Afficher les premiers éléments de l'ensemble de données\n",
    "# for img, label in dataset.take(5):\n",
    "#     print(img.shape, label)  # A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7221 files belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset = loadDataset(\"increased/leaves/images\", img_size, batch_size)\n",
    "train_ds, validation_ds = get_dataset_partition_tf(dataset)\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(\n",
    "    buffer_size=tf.data.AUTOTUNE)\n",
    "validation_ds = validation_ds.cache().shuffle(1000).prefetch(\n",
    "    buffer_size=tf.data.AUTOTUNE)\n",
    "class_names = dataset.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "            Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Conv2D(32, (3, 3), activation='relu'),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Flatten(),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(len(class_names), activation='softmax')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(input_shape=input_shape)\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=False),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs,\n",
    "    batch_size = 32,\n",
    "    verbose=1,\n",
    "    validation_data=validation_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
